{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78751d97-0d23-4f85-892f-a6ed8515a7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark.sql(\"USE CATALOG midterm\")\n",
    "spark.sql(\"USE SCHEMA source1_layer\")\n",
    "\n",
    "# ============================================\n",
    "# DIM_RESTAURANT (SCD TYPE 2) - FIXED\n",
    "# ============================================\n",
    "\n",
    "@dlt.table(name=\"bronze_restaurant_cdf\")\n",
    "def bronze_restaurant_cdf():\n",
    "    \"\"\"Extract restaurant attributes with CLEANED license_no\"\"\"\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df = df.select(\n",
    "        col(\"License_No\"),\n",
    "        col(\"DBA_Name\"),\n",
    "        col(\"AKA_Name\"),\n",
    "        col(\"Facility_Type\"),\n",
    "        col(\"Risk_Category\"),\n",
    "        col(\"City\"),\n",
    "        col(\"FileName\")\n",
    "    )\n",
    "    \n",
    "    # ✅ Remove city suffix from License_No\n",
    "    df = df.withColumn(\"License_No\",\n",
    "        regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    regexp_replace(col(\"License_No\"), \"_DALLAS$\", \"\"),\n",
    "                    \"_CHICAGO$\", \"\"),\n",
    "                \"_Dallas$\", \"\"),\n",
    "            \"_Chicago$\", \"\"))\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df = df.dropDuplicates([\"License_No\", \"DBA_Name\", \"AKA_Name\", \n",
    "                            \"Facility_Type\", \"Risk_Category\", \"City\"])\n",
    "    \n",
    "    # Add CDC metadata\n",
    "    df = df.withColumn(\"_commit_timestamp\", current_timestamp()) \\\n",
    "           .withColumn(\"_change_type\", lit(\"insert\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "@dlt.view\n",
    "def silver_restaurant_cdf():\n",
    "    df = spark.readStream.table(\"LIVE.bronze_restaurant_cdf\")\n",
    "    \n",
    "    return (df\n",
    "        .withColumn(\"License_No\", trim(col(\"License_No\")))\n",
    "        .withColumn(\"DBA_Name\", trim(col(\"DBA_Name\")))\n",
    "        .withColumn(\"AKA_Name\", \n",
    "            when(col(\"AKA_Name\").isNull() | (trim(col(\"AKA_Name\")) == \"\"), \n",
    "                 trim(col(\"DBA_Name\")))\n",
    "            .otherwise(trim(col(\"AKA_Name\"))))\n",
    "        .withColumn(\"Facility_Type\", trim(col(\"Facility_Type\")))\n",
    "        .withColumn(\"Risk_Category\",\n",
    "            when(col(\"Risk_Category\").isNull() | (trim(col(\"Risk_Category\")) == \"\"),\n",
    "                 lit(\"UNKNOWN\"))\n",
    "            .otherwise(trim(col(\"Risk_Category\"))))\n",
    "        .withColumn(\"City\", trim(upper(col(\"City\"))))\n",
    "    )\n",
    "\n",
    "dlt.create_streaming_table(name=\"restaurant_cdf_type2_stage\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target=\"restaurant_cdf_type2_stage\",\n",
    "    source=\"silver_restaurant_cdf\",\n",
    "    keys=[\"License_No\", \"City\"],\n",
    "    sequence_by=col(\"_commit_timestamp\"),\n",
    "    ignore_null_updates=True,\n",
    "    apply_as_deletes=expr(\"_change_type = 'delete'\"),\n",
    "    stored_as_scd_type=2\n",
    ")\n",
    "\n",
    "@dlt.view\n",
    "def silver_restaurant_dim_final():\n",
    "    \"\"\"Transform stage to final format - FIXED for new DLT version\"\"\"\n",
    "    df = spark.read.table(\"LIVE.restaurant_cdf_type2_stage\")\n",
    "    \n",
    "    return (df\n",
    "        # ✅ FIX: __START_AT and __END_AT are now TIMESTAMP directly, not structs\n",
    "        .withColumn(\"effective_date\", to_date(col(\"__START_AT\")))  # Changed!\n",
    "        .withColumn(\"end_date\", \n",
    "            when(col(\"__END_AT\").isNull(), to_date(lit(\"9999-12-31\")))  # Changed!\n",
    "            .otherwise(to_date(col(\"__END_AT\"))))  # Changed!\n",
    "        .withColumn(\"is_current\", \n",
    "            when(col(\"__END_AT\").isNull(), lit(True))  # Changed!\n",
    "            .otherwise(lit(False)))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_restaurant_scd2\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .withColumnRenamed(\"License_No\", \"license_no\")\n",
    "        .withColumnRenamed(\"DBA_Name\", \"dba_name\")\n",
    "        .withColumnRenamed(\"AKA_Name\", \"aka_name\")\n",
    "        .withColumnRenamed(\"Facility_Type\", \"facility_type\")\n",
    "        .withColumnRenamed(\"Risk_Category\", \"risk_category\")\n",
    "        .withColumnRenamed(\"City\", \"city\")\n",
    "        .withColumnRenamed(\"FileName\", \"file_name\")\n",
    "        .drop(\"__START_AT\", \"__END_AT\", \"_commit_timestamp\", \"_change_type\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"dim_restaurant\",\n",
    "    partition_cols=[\"is_current\"]\n",
    ")\n",
    "def dim_restaurant():\n",
    "    df = spark.read.table(\"LIVE.silver_restaurant_dim_final\")\n",
    "    \n",
    "    window_spec = Window.partitionBy(lit(1)).orderBy(\"license_no\", \"city\", \"effective_date\")\n",
    "    df = df.withColumn(\"restaurant_key\", row_number().over(window_spec))\n",
    "    \n",
    "    return df.select(\n",
    "        \"restaurant_key\", \"license_no\", \"dba_name\", \"aka_name\", \n",
    "        \"facility_type\", \"risk_category\", \"city\", \"effective_date\", \n",
    "        \"end_date\", \"is_current\", \"file_name\", \"job_load_id\", \"job_load_date\"\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# OTHER DIMENSIONS (Keep as before)\n",
    "# ============================================\n",
    "@dlt.table(name=\"dim_date\")\n",
    "def dim_date():\n",
    "    start_date = \"2018-01-01\"\n",
    "    end_date = \"2028-12-31\"\n",
    "    \n",
    "    date_df = spark.sql(f\"\"\"\n",
    "        SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as full_date\n",
    "    \"\"\")\n",
    "    \n",
    "    return (\n",
    "        date_df\n",
    "        .withColumn(\"date_key\", concat(lpad(year(col(\"full_date\")), 4, \"0\"),\n",
    "                                       lpad(month(col(\"full_date\")), 2, \"0\"),\n",
    "                                       lpad(dayofmonth(col(\"full_date\")), 2, \"0\")).cast(\"int\"))\n",
    "        .withColumn(\"year\", year(col(\"full_date\")))\n",
    "        .withColumn(\"quarter\", quarter(col(\"full_date\")))\n",
    "        .withColumn(\"month\", month(col(\"full_date\")))\n",
    "        .withColumn(\"month_name\", date_format(col(\"full_date\"), \"MMMM\"))\n",
    "        .withColumn(\"day\", dayofmonth(col(\"full_date\")))\n",
    "        .withColumn(\"day_of_week\", dayofweek(col(\"full_date\")))\n",
    "        .withColumn(\"day_name\", date_format(col(\"full_date\"), \"EEEE\"))\n",
    "        .withColumn(\"week_of_year\", weekofyear(col(\"full_date\")))\n",
    "        .withColumn(\"is_weekend\", when(dayofweek(col(\"full_date\")).isin([1, 7]), True).otherwise(False))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_date_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .select(\"date_key\", \"full_date\", \"year\", \"quarter\", \"month\", \"month_name\",\n",
    "                \"day\", \"day_of_week\", \"day_name\", \"week_of_year\", \"is_weekend\",\n",
    "                \"job_load_id\", \"job_load_date\")\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"dim_location\")\n",
    "def dim_location():\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df_location = (df\n",
    "        .select(\n",
    "            trim(col(\"Address\")).alias(\"address\"),\n",
    "            trim(upper(col(\"City\"))).alias(\"city\"),\n",
    "            col(\"Zip_Code\").cast(\"int\").alias(\"zip_code\"),\n",
    "            col(\"Latitude\").alias(\"latitude\"),\n",
    "            col(\"Longitude\").alias(\"longitude\")\n",
    "        )\n",
    "        .filter(col(\"address\").isNotNull())\n",
    "        .filter(col(\"zip_code\").isNotNull())\n",
    "        .withColumn(\"location_business_key\", \n",
    "            md5(concat_ws(\"|\", lower(col(\"address\")), lower(col(\"city\")), col(\"zip_code\"))))\n",
    "        .dropDuplicates([\"location_business_key\"])\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.orderBy(\"address\", \"city\", \"zip_code\")\n",
    "    return (df_location\n",
    "        .withColumn(\"location_key\", row_number().over(window_spec))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_location_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .select(\"location_key\", \"location_business_key\", \"address\", \"city\",\n",
    "                \"zip_code\", \"latitude\", \"longitude\", \"job_load_id\", \"job_load_date\")\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"dim_inspection_type\")\n",
    "def dim_inspection_type():\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df_type = (df\n",
    "        .select(trim(col(\"Inspection_Type\")).alias(\"inspection_type\"),\n",
    "                trim(upper(col(\"City\"))).alias(\"city\"))\n",
    "        .filter(col(\"inspection_type\").isNotNull())\n",
    "        .withColumn(\"inspection_type_business_key\",\n",
    "            md5(concat_ws(\"|\", col(\"inspection_type\"), col(\"city\"))))\n",
    "        .dropDuplicates([\"inspection_type_business_key\"])\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.orderBy(\"inspection_type\", \"city\")\n",
    "    return (df_type\n",
    "        .withColumn(\"inspection_type_key\", row_number().over(window_spec))\n",
    "        .withColumn(\"inspection_category\",\n",
    "            when(lower(col(\"inspection_type\")).contains(\"routine\"), \"Routine\")\n",
    "            .when(lower(col(\"inspection_type\")).contains(\"follow\"), \"Follow-up\")\n",
    "            .when(lower(col(\"inspection_type\")).contains(\"complaint\"), \"Complaint\")\n",
    "            .otherwise(\"Other\"))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_inspection_type_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .select(\"inspection_type_key\", \"inspection_type_business_key\", \n",
    "                \"inspection_type\", \"inspection_category\", \"city\", \n",
    "                \"job_load_id\", \"job_load_date\")\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"dim_inspection_result\")\n",
    "def dim_inspection_result():\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df_result = (df\n",
    "        .select(trim(col(\"Inspection_Results\")).alias(\"result_code\"))\n",
    "        .filter(col(\"result_code\").isNotNull())\n",
    "        .withColumn(\"result_business_key\", md5(col(\"result_code\")))\n",
    "        .dropDuplicates([\"result_business_key\"])\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.orderBy(\"result_code\")\n",
    "    return (df_result\n",
    "        .withColumn(\"inspection_result_key\", row_number().over(window_spec))\n",
    "        .withColumn(\"result_category\",\n",
    "            when(upper(col(\"result_code\")).contains(\"PASS\"), \"Pass\")\n",
    "            .when(upper(col(\"result_code\")).contains(\"FAIL\"), \"Fail\")\n",
    "            .otherwise(\"Other\"))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_inspection_result_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .select(\"inspection_result_key\", \"result_business_key\", \"result_code\", \n",
    "                \"result_category\", \"job_load_id\", \"job_load_date\")\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"dim_risk_category\")\n",
    "def dim_risk_category():\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df_risk = (df\n",
    "        .select(col(\"Risk_Category\").alias(\"risk_category\"))\n",
    "        .filter(col(\"risk_category\").isNotNull())\n",
    "        .withColumn(\"risk_level\",\n",
    "            when(lower(col(\"risk_category\")).contains(\"high\") | \n",
    "                 lower(col(\"risk_category\")).contains(\"1\"), \"High\")\n",
    "            .when(lower(col(\"risk_category\")).contains(\"medium\") | \n",
    "                  lower(col(\"risk_category\")).contains(\"2\"), \"Medium\")\n",
    "            .when(lower(col(\"risk_category\")).contains(\"low\") | \n",
    "                  lower(col(\"risk_category\")).contains(\"3\"), \"Low\")\n",
    "            .otherwise(\"Unknown\"))\n",
    "        .withColumn(\"priority_level\",\n",
    "            when(col(\"risk_level\") == \"High\", 1)\n",
    "            .when(col(\"risk_level\") == \"Medium\", 2)\n",
    "            .when(col(\"risk_level\") == \"Low\", 3)\n",
    "            .otherwise(99))\n",
    "        .select(\"risk_level\", \"priority_level\")\n",
    "        .distinct()\n",
    "        .withColumn(\"risk_business_key\",\n",
    "            md5(concat_ws(\"|\", col(\"risk_level\"), col(\"priority_level\").cast(\"string\"))))\n",
    "        .dropDuplicates([\"risk_business_key\"])\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.orderBy(\"priority_level\")\n",
    "    return (df_risk\n",
    "        .withColumn(\"risk_category_key\", row_number().over(window_spec))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_risk_category_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .select(\"risk_category_key\", \"risk_business_key\", \"risk_level\", \n",
    "                \"priority_level\", \"job_load_id\", \"job_load_date\")\n",
    "    )\n",
    "\n",
    "@dlt.table(name=\"dim_violation\")\n",
    "def dim_violation():\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df_violation = (df\n",
    "        .select(\n",
    "            col(\"Violation_Code\").cast(\"int\").alias(\"violation_code\"),\n",
    "            trim(col(\"Violation_Desc\")).alias(\"violation_desc\"),\n",
    "            coalesce(col(\"is_violation_critical\"), lit(False)).alias(\"is_violation_critical\"),\n",
    "            coalesce(col(\"is_violation_urgent\"), lit(False)).alias(\"is_violation_urgent\"),\n",
    "            col(\"City\").alias(\"city_source\")\n",
    "        )\n",
    "        .filter(col(\"violation_code\").isNotNull())\n",
    "        .filter(col(\"violation_desc\").isNotNull())\n",
    "        .groupBy(\"violation_code\", \"city_source\")\n",
    "        .agg(\n",
    "            first(\"violation_desc\").alias(\"violation_desc\"),\n",
    "            max(\"is_violation_critical\").alias(\"is_violation_critical\"),\n",
    "            max(\"is_violation_urgent\").alias(\"is_violation_urgent\")\n",
    "        )\n",
    "        .withColumn(\"violation_business_key\",\n",
    "            md5(concat_ws(\"|\", col(\"violation_code\"), col(\"city_source\"))))\n",
    "        .dropDuplicates([\"violation_business_key\"])\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.orderBy(\"violation_code\", \"city_source\")\n",
    "    return (df_violation\n",
    "        .withColumn(\"violation_key\", row_number().over(window_spec))\n",
    "        .withColumn(\"violation_category\",\n",
    "            when(col(\"is_violation_critical\") == True, \"Critical\")\n",
    "            .when(col(\"is_violation_urgent\") == True, \"Urgent\")\n",
    "            .otherwise(\"Standard\"))\n",
    "        .withColumn(\"job_load_id\", lit(\"dim_violation_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .select(\"violation_key\", \"violation_business_key\", \"violation_code\", \n",
    "                \"violation_desc\", \"is_violation_critical\", \"is_violation_urgent\", \n",
    "                \"violation_category\", \"city_source\", \"job_load_id\", \"job_load_date\")\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# FACT_INSPECTION (COMPLETE)\n",
    "# ============================================\n",
    "@dlt.table(\n",
    "    name=\"fact_inspection\",\n",
    "    partition_cols=[\"date_key\"]\n",
    ")\n",
    "def fact_inspection():\n",
    "    \n",
    "    df_silver = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    # Prepare fact data\n",
    "    df_fact = df_silver.select(\n",
    "        col(\"Inspection_ID\").alias(\"inspection_id\"),\n",
    "        col(\"Inspection_Date\").alias(\"inspection_date\"),\n",
    "        col(\"License_No\"),\n",
    "        col(\"Address\"),\n",
    "        col(\"City\"),\n",
    "        col(\"Zip_Code\").cast(\"int\").alias(\"zip_code\"),\n",
    "        trim(col(\"Inspection_Type\")).alias(\"inspection_type\"),\n",
    "        trim(col(\"Inspection_Results\")).alias(\"inspection_results\"),\n",
    "        col(\"Risk_Category\").alias(\"risk_category\"),\n",
    "        col(\"Inspection_Score\").cast(\"int\").alias(\"inspection_score\"),\n",
    "        col(\"Violation_Count\").cast(\"int\").alias(\"violation_count\"),\n",
    "        col(\"FileName\").alias(\"file_name\")\n",
    "    ).dropDuplicates([\"Inspection_ID\"])\n",
    "    \n",
    "    # ✅ Remove city suffix from License_No (same as dim_restaurant)\n",
    "    df_fact = df_fact.withColumn(\"clean_license\",\n",
    "        trim(regexp_replace(\n",
    "            regexp_replace(\n",
    "                regexp_replace(\n",
    "                    regexp_replace(col(\"License_No\"), \"_DALLAS$\", \"\"),\n",
    "                    \"_CHICAGO$\", \"\"),\n",
    "                \"_Dallas$\", \"\"),\n",
    "            \"_Chicago$\", \"\")))\n",
    "    \n",
    "    # Clean other columns\n",
    "    df_fact = df_fact \\\n",
    "        .withColumn(\"clean_city\", trim(upper(col(\"City\")))) \\\n",
    "        .withColumn(\"clean_address\", trim(col(\"Address\")))\n",
    "    \n",
    "    # Create date_key\n",
    "    df_fact = df_fact.withColumn(\"date_key\",\n",
    "        concat(\n",
    "            lpad(year(col(\"inspection_date\")), 4, \"0\"),\n",
    "            lpad(month(col(\"inspection_date\")), 2, \"0\"),\n",
    "            lpad(dayofmonth(col(\"inspection_date\")), 2, \"0\")\n",
    "        ).cast(\"int\"))\n",
    "    \n",
    "    # Derive risk_level\n",
    "    df_fact = df_fact.withColumn(\"risk_level\",\n",
    "        when(lower(col(\"risk_category\")).contains(\"high\") | \n",
    "             lower(col(\"risk_category\")).contains(\"1\"), \"High\")\n",
    "        .when(lower(col(\"risk_category\")).contains(\"medium\") | \n",
    "              lower(col(\"risk_category\")).contains(\"2\"), \"Medium\")\n",
    "        .when(lower(col(\"risk_category\")).contains(\"low\") | \n",
    "              lower(col(\"risk_category\")).contains(\"3\"), \"Low\")\n",
    "        .otherwise(\"Unknown\"))\n",
    "    \n",
    "    # Create business keys\n",
    "    df_fact = df_fact \\\n",
    "        .withColumn(\"loc_bk\", md5(concat_ws(\"|\", \n",
    "            lower(col(\"clean_address\")), \n",
    "            lower(col(\"clean_city\")), \n",
    "            col(\"zip_code\")))) \\\n",
    "        .withColumn(\"type_bk\", md5(concat_ws(\"|\", col(\"inspection_type\"), col(\"clean_city\")))) \\\n",
    "        .withColumn(\"result_bk\", md5(col(\"inspection_results\"))) \\\n",
    "        .withColumn(\"risk_bk\", md5(concat_ws(\"|\", col(\"risk_level\"),\n",
    "            when(col(\"risk_level\") == \"High\", 1)\n",
    "            .when(col(\"risk_level\") == \"Medium\", 2)\n",
    "            .when(col(\"risk_level\") == \"Low\", 3)\n",
    "            .otherwise(99).cast(\"string\"))))\n",
    "    \n",
    "    # Read dimensions\n",
    "    dim_restaurant = spark.read.table(\"LIVE.dim_restaurant\").filter(col(\"is_current\") == True)\n",
    "    dim_location = spark.read.table(\"LIVE.dim_location\")\n",
    "    dim_type = spark.read.table(\"LIVE.dim_inspection_type\")\n",
    "    dim_result = spark.read.table(\"LIVE.dim_inspection_result\")\n",
    "    dim_risk = spark.read.table(\"LIVE.dim_risk_category\")\n",
    "    \n",
    "    # JOIN Restaurant\n",
    "    df_fact = df_fact.join(\n",
    "        broadcast(dim_restaurant.select(\n",
    "            col(\"license_no\").alias(\"r_lic\"),\n",
    "            col(\"city\").alias(\"r_city\"),\n",
    "            col(\"restaurant_key\")\n",
    "        )),\n",
    "        (col(\"clean_license\") == col(\"r_lic\")) & \n",
    "        (col(\"clean_city\") == col(\"r_city\")),\n",
    "        \"left\"\n",
    "    ).drop(\"r_lic\", \"r_city\")\n",
    "    \n",
    "    # JOIN Location\n",
    "    df_fact = df_fact.join(\n",
    "        broadcast(dim_location.select(col(\"location_business_key\"), col(\"location_key\"))),\n",
    "        df_fact[\"loc_bk\"] == dim_location[\"location_business_key\"],\n",
    "        \"left\"\n",
    "    ).drop(\"loc_bk\")\n",
    "    \n",
    "    # JOIN Type\n",
    "    df_fact = df_fact.join(\n",
    "        broadcast(dim_type.select(col(\"inspection_type_business_key\"), col(\"inspection_type_key\"))),\n",
    "        df_fact[\"type_bk\"] == dim_type[\"inspection_type_business_key\"],\n",
    "        \"left\"\n",
    "    ).drop(\"type_bk\")\n",
    "    \n",
    "    # JOIN Result\n",
    "    df_fact = df_fact.join(\n",
    "        broadcast(dim_result.select(col(\"result_business_key\"), col(\"inspection_result_key\"))),\n",
    "        df_fact[\"result_bk\"] == dim_result[\"result_business_key\"],\n",
    "        \"left\"\n",
    "    ).drop(\"result_bk\")\n",
    "    \n",
    "    # JOIN Risk\n",
    "    df_fact = df_fact.join(\n",
    "        broadcast(dim_risk.select(col(\"risk_business_key\"), col(\"risk_category_key\"))),\n",
    "        df_fact[\"risk_bk\"] == dim_risk[\"risk_business_key\"],\n",
    "        \"left\"\n",
    "    ).drop(\"risk_bk\")\n",
    "    \n",
    "    # Add surrogate key\n",
    "    window_spec = Window.orderBy(\"inspection_id\")\n",
    "    df_fact = df_fact.withColumn(\"inspection_fact_key\", row_number().over(window_spec))\n",
    "    \n",
    "    # Add audit\n",
    "    df_fact = df_fact \\\n",
    "        .withColumn(\"job_load_id\", lit(\"fact_inspection_load\")) \\\n",
    "        .withColumn(\"job_load_date\", current_timestamp()) \\\n",
    "        .withColumn(\"created_date\", current_timestamp())\n",
    "    \n",
    "    return df_fact.select(\n",
    "        \"inspection_fact_key\", \"inspection_id\", \"date_key\",\n",
    "        \"restaurant_key\", \"location_key\", \"inspection_type_key\",\n",
    "        \"inspection_result_key\", \"risk_category_key\",\n",
    "        \"inspection_score\", \"violation_count\", \"file_name\",\n",
    "        \"inspection_date\", \"created_date\", \"job_load_id\", \"job_load_date\"\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# FACT_VIOLATION\n",
    "# ============================================\n",
    "@dlt.table(name=\"bridge_fact_violation\")\n",
    "def fact_violation():\n",
    "    df = spark.read.table(\"midterm.source1_layer.silver_table\")\n",
    "    \n",
    "    df_violation = (df\n",
    "        .select(\n",
    "            col(\"Inspection_ID\").alias(\"inspection_id\"),\n",
    "            col(\"Violation_Code\").cast(\"int\").alias(\"violation_code\"),\n",
    "            col(\"City\").alias(\"city_source\"),\n",
    "            trim(col(\"Violation_Comments\")).alias(\"violation_comments\")\n",
    "        )\n",
    "        .filter(col(\"violation_code\").isNotNull())\n",
    "        .withColumn(\"violation_business_key\",\n",
    "            md5(concat_ws(\"|\", col(\"violation_code\"), col(\"city_source\"))))\n",
    "    )\n",
    "    \n",
    "    fact_inspection = spark.read.table(\"LIVE.fact_inspection\")\n",
    "    dim_violation = spark.read.table(\"LIVE.dim_violation\")\n",
    "    \n",
    "    df_violation = (df_violation\n",
    "        .join(fact_inspection.select(\"inspection_id\", col(\"inspection_fact_key\")), \n",
    "              \"inspection_id\", \"inner\")\n",
    "        .join(dim_violation.select(\"violation_business_key\", col(\"violation_key\")), \n",
    "              \"violation_business_key\", \"left\")\n",
    "    )\n",
    "    \n",
    "    window_spec = Window.orderBy(\"inspection_id\", \"violation_code\")\n",
    "    return (df_violation\n",
    "        .withColumn(\"violation_fact_key\", row_number().over(window_spec))\n",
    "        .withColumn(\"job_load_id\", lit(\"fact_violation_initial_load\"))\n",
    "        .withColumn(\"job_load_date\", current_timestamp())\n",
    "        .withColumn(\"created_date\", current_timestamp())\n",
    "        .select(\n",
    "            \"violation_fact_key\", \"inspection_fact_key\", \"violation_key\",\n",
    "            \"violation_comments\", \"created_date\", \"job_load_id\", \"job_load_date\")\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf23c0c-b4a5-4d91-b251-f628b549e728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_2_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
